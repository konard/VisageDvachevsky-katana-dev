name: Performance Regression Detection

on:
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'katana/core/**'
      - 'benchmark/**'
      - 'benchmarks/baseline.json'
  workflow_dispatch:
    inputs:
      threshold:
        description: 'Regression threshold percentage'
        required: false
        default: '3'
      run_full_benchmarks:
        description: 'Run full benchmark suite'
        required: false
        default: 'false'

env:
  REGRESSION_THRESHOLD: ${{ github.event.inputs.threshold || '3' }}

jobs:
  benchmark-regression:
    name: Performance Regression Check
    runs-on: ubuntu-24.04
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake ninja-build clang-18 clang++-18 clang-tools-18 \
          liburing-dev libc++-18-dev libc++abi-18-dev ccache python3 python3-pip
        pip install --no-cache-dir matplotlib numpy

    - name: Cache CMake build
      uses: actions/cache@v4
      with:
        path: |
          build
          ~/.cache/ccache
        key: ${{ runner.os }}-benchmark-${{ hashFiles('**/CMakeLists.txt') }}-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-benchmark-${{ hashFiles('**/CMakeLists.txt') }}-
          ${{ runner.os }}-benchmark-

    - name: Configure and Build Benchmarks
      run: |
        SCAN_DEPS_FLAGS="-DCMAKE_C_COMPILER_CLANG_SCAN_DEPS=/usr/bin/clang-scan-deps-18 -DCMAKE_CXX_COMPILER_CLANG_SCAN_DEPS=/usr/bin/clang-scan-deps-18"
        cmake -B build \
          -G Ninja \
          -DCMAKE_BUILD_TYPE=Release \
          -DCMAKE_C_COMPILER=clang-18 \
          -DCMAKE_CXX_COMPILER=clang++-18 \
          -DCMAKE_C_COMPILER_LAUNCHER=ccache \
          -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \
          -DENABLE_BENCHMARKS=ON \
          ${SCAN_DEPS_FLAGS}
        cmake --build build --target simple_benchmark performance_benchmark -j$(nproc)

    - name: Run Core Benchmarks
      id: run_benchmarks
      run: |
        mkdir -p benchmark_results

        # Run benchmarks and capture output
        echo "Running simple_benchmark..."
        timeout 120 ./build/benchmark/simple_benchmark 2>&1 | tee benchmark_results/simple_benchmark.log || true

        echo "Running performance_benchmark (quick mode)..."
        timeout 120 ./build/benchmark/performance_benchmark 2>&1 | tee benchmark_results/performance_benchmark.log || true

    - name: Parse Benchmark Results
      run: |
        python3 -c "
        import json
        import re
        import os

        results = {}

        def parse_benchmark_line(line):
            patterns = [
                r'(\w[\w\s]+):\s+([\d.]+)\s*(ops/sec|req/s|us|ms|ns)',
                r'BM_(\w+)\s+([\d.]+)\s*(ns|us|ms|s)',
            ]
            for pattern in patterns:
                match = re.search(pattern, line)
                if match:
                    name = match.group(1).strip().replace(' ', '_').lower()
                    value = float(match.group(2))
                    unit = match.group(3)
                    return name, value, unit
            return None, None, None

        for log_file in ['simple_benchmark.log', 'performance_benchmark.log']:
            path = f'benchmark_results/{log_file}'
            if os.path.exists(path):
                with open(path) as f:
                    for line in f:
                        name, value, unit = parse_benchmark_line(line)
                        if name:
                            results[name] = {'value': value, 'unit': unit}

        with open('benchmark_results/current.json', 'w') as f:
            json.dump(results, f, indent=2)

        print(f'Parsed {len(results)} metrics')
        "

    - name: Compare Against Baseline
      id: compare
      run: |
        if [ -f "benchmarks/baseline.json" ]; then
          echo "Comparing against baseline with threshold: ${REGRESSION_THRESHOLD}%"
          python3 scripts/compare_benchmarks.py \
            --baseline benchmarks/baseline.json \
            --current benchmark_results/current.json \
            --threshold "${REGRESSION_THRESHOLD}" \
            --output benchmark_results/comparison.json \
            --no-fail || REGRESSION_DETECTED=1

          if [ "${REGRESSION_DETECTED}" = "1" ]; then
            echo "regression_detected=true" >> $GITHUB_OUTPUT
            echo "::warning::Performance regression detected! See comparison results."
          else
            echo "regression_detected=false" >> $GITHUB_OUTPUT
            echo "::notice::No performance regression detected."
          fi
        else
          echo "No baseline.json found, skipping comparison"
          echo "regression_detected=false" >> $GITHUB_OUTPUT
        fi

    - name: Upload Benchmark Results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark_results/
        retention-days: 30

    - name: Post Comment on PR
      if: github.event_name == 'pull_request' && steps.compare.outputs.regression_detected == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          let comparison = {};
          try {
            comparison = JSON.parse(fs.readFileSync('benchmark_results/comparison.json', 'utf8'));
          } catch (e) {
            console.log('Could not read comparison.json');
            return;
          }

          let body = '## ⚠️ Performance Regression Detected\n\n';
          body += `Threshold: ${process.env.REGRESSION_THRESHOLD}%\n\n`;

          if (comparison.comparisons) {
            const regressions = comparison.comparisons.filter(c => c.is_regression);
            if (regressions.length > 0) {
              body += '### Regressions\n\n';
              body += '| Metric | Baseline | Current | Change |\n';
              body += '|--------|----------|---------|--------|\n';
              regressions.forEach(r => {
                body += `| ${r.name} | ${r.baseline.toFixed(2)} | ${r.current.toFixed(2)} | ${r.change_percent.toFixed(2)}% |\n`;
              });
            }
          }

          body += '\n---\n*This check runs automatically on changes to core code.*';

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });

  # Optional: Full benchmark suite (only on workflow_dispatch with flag)
  full-benchmarks:
    name: Full Benchmark Suite
    runs-on: ubuntu-24.04
    if: github.event.inputs.run_full_benchmarks == 'true'
    timeout-minutes: 60

    steps:
    - uses: actions/checkout@v4

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake ninja-build clang-18 clang++-18 clang-tools-18 \
          liburing-dev libc++-18-dev libc++abi-18-dev ccache wrk

    - name: Build all benchmarks
      run: |
        SCAN_DEPS_FLAGS="-DCMAKE_C_COMPILER_CLANG_SCAN_DEPS=/usr/bin/clang-scan-deps-18 -DCMAKE_CXX_COMPILER_CLANG_SCAN_DEPS=/usr/bin/clang-scan-deps-18"
        cmake -B build \
          -G Ninja \
          -DCMAKE_BUILD_TYPE=Release \
          -DCMAKE_C_COMPILER=clang-18 \
          -DCMAKE_CXX_COMPILER=clang++-18 \
          -DENABLE_BENCHMARKS=ON \
          ${SCAN_DEPS_FLAGS}
        cmake --build build -j$(nproc)

    - name: Run Full Benchmark Suite
      run: |
        mkdir -p benchmark_results

        for bench in simple_benchmark performance_benchmark router_benchmark io_buffer_benchmark timer_benchmark; do
          if [ -f "build/benchmark/$bench" ]; then
            echo "Running $bench..."
            timeout 300 ./build/benchmark/$bench 2>&1 | tee "benchmark_results/${bench}.log" || true
          fi
        done

    - name: Upload Full Results
      uses: actions/upload-artifact@v4
      with:
        name: full-benchmark-results
        path: benchmark_results/
        retention-days: 90
